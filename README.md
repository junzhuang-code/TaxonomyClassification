# Understanding Survey Paper Taxonomy about Large Language Models via Graph Representation Learning

### Authors: Jun Zhuang, Casey Kennington

### Paper:
> The 4th Workshop on Scholarly Document Processing @ ACL 2024

### Abstract:
> <p align="justify">
As new research on Large Language Models (LLMs) continues, it is difficult to keep up with new research and models. To help researchers synthesize the new research many have written survey papers, but even those have become numerous. In this paper, we develop a method to automatically assign survey papers to a taxonomy. We collect the metadata of 144 LLM survey papers and explore three paradigms to classify papers within the taxonomy. Our work indicates that leveraging graph structure information on co-category graphs can significantly outperform the language models in two paradigms; pre-trained language models' fine-tuning and zero-shot/few-shot classifications using LLMs. We find that our model surpasses an average human recognition level and that fine-tuning LLMs using weak labels generated by a smaller model, such as the GCN in this study, can be more effective than using ground-truth labels, revealing the potential of weak-to-strong generalization in the taxonomy classification task.
</p>

### Dataset:
> We first collected the metadata of 144 literature reviews about Large Language Models (LLMs). \
> We build three subgraphs for validating our datasets via both GNNs and LMs. \
> We summarized the LLMs survey papers at [LLMSurveySummary](https://github.com/junzhuang-code/LLMSurveySummary).

### Getting Started:
#### Prerequisites
> Linux or macOS \
> CPU or NVIDIA GPU + CUDA CuDNN \
> Python 3.11 \
> pytorch, dgl, transformers, numpy, scipy, sklearn, yaml

#### Clone this repo
> ```git clone https://github.com/junzhuang-code/TaxonomyClassification.git``` \
> ```cd TaxonomyClassification```

#### Install dependencies
> For pip users, please type the command: ```pip install -r requirements.txt``` \
> For Conda users, you may create a new Conda environment using: ```conda env create -f environment.yml```

#### Directories
> **data**: contain graph data (to be created), survey data, and text corpus; \
> **data_collection**: contain scripts for scraping data and processing text corpus; \
> **baselines**: source code of GNNs and LMs; \
> **config**: the config files.

#### Processing Steps
> **Collecting meta data**: Collect raw data from arXiv. \
> **Constructing graph data**: Generate co-graphs and text graphs. \
> **Evaluating the dataset**: Validate the dataset via GNNs and LMs.
